We thank the reviewers for their comments.  We will work hard to
improve the presentation of the paper. We will act on all detailed
comments.

Reviewers 2, 3 and 4 ask about mechanisation. We agree that
mechanisation would be a good thing. It is a big undertaking, as
mentioned by reviewer 2, and can only be done now that theory has been
achieved. In fact, our immediate goal is to provide a bounded model
checking tool for our operational semantics to check invariants up to
a bound. Our aim is to check the two large examples, TPC-C [47] and
RUBiS [41], which we highlight as being too big to prove by hand in
the related work. Another goal is to generate litmus tests to identify
when implementations, such as Amazon's Dynamo DB, do not satisfy a
particular consistency model.

Reviewer 1: length of paper and appendix.

We follow the submission conditions: the page limit is 25 pages plus
bibliography. The comment is really about the readability and content
of the paper and appendix. The key result of the paper is that our
definitions of consistency model relate to known axiomatic definitions
in the literature.  We will put the paper on arXiv with an appendix
which contains the details of this relationship: appendix A, C, D, E
and F. We do not need appendix B on dependency graphs since our
results use abstract executions and refer to [17] relating abstract
executions with dependency graphs. Our reason for having both
applications is to illustrate that our operational semantics provides
a natural interface between protocols  and clients. We will think
about the balance of this section carefully. We can make the
applications more self-contained in the paper, and refer to Xiong's
thesis (to be submitted on January 6th) for the details (appendix G and
H). This is a different solution to the one proposed, which we think
will work better, but we are happy to take further advice on this.

Thank you for saying that the appendix contains one (if not several)
papers of its own. 

Reviewer 3: comparison with [19].

The abstract state in our framework does not keep track of the whole
history but only the client-observable history. For example, our
abstract state does not contain any information about the order of
execution of transactions that access disjoint keys. In contrast, [19]
keeps this information in the state.  With frameworks that carry
around the total execution order ([19] and abstract executions), there
is evidence to suggest that client reasoning requires the inference of 
the (client-)observable behaviour in term of dependency information 
between transactions (the write-read, write-write and read-write 
relations of dependency graphs) [10, 17, 18, 37]. 
This dependency information is determined by the consistency
model, and to the best of our knowledge it is not known whether there
exists an automatic procedure to retrieve it ([17] shows that it is
possible for some, but leaves the question open for all). Our
execution tests are built directly on top of client-observable
histories, so the dependency information is immediate.
