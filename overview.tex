\newcommand{\Counter}{\ensuremath{\mathsf{Counter}}}
\newcommand{\ctrinc}{\ensuremath{\mathsf{inc}}}
\newcommand{\ctrread}{\ensuremath{\mathsf{read}}}
%



\section{Overview}
\label{sec:overview}


We introduce an interleaving operational semantics for describing the
client-observable behaviour of atomic transactions on distributed
kv-stores. Our semantics builds on abstract states comprising
centralised, global kv-stores and partial client views.  We provide
operational definitions of consistency model for our kv-stores which
are shown to be equivalent to the well-known declarative definitions
of consistency model for execution graphs. We have identified two
immediate applications of our semantics: specific protocols
geo-replicated databases (e.g. COPS) and partitioned databases
(e.g. Clock-SI) can be shown to be correct for a specific consistency
model by embedding them in our centralised semantics; general
robustness results for clients can be proved with respect to a
specific consistency model. ...we expect many more applications in
future.....




\mypar{Example} We use a simple
$\mathsf{Counter}(\key)$ example to help  introduce our operational semantics.  Clients can manipulate the value of key
$\key$ via two transactions:

\vspace{-5pt}
{%
\displaymathfont
\[%
\begin{array}{r @{\hspace{2pt}} l @{\hspace{18pt}} r @{\hspace{2pt}} l}
\ctrinc(\key) \defeq 
&
\begin{session}
\begin{transaction}
\plookup{\pv{x}}{\key}; \ 
\pmutate{\key}{\pv{x}+1}
\end{transaction}
\end{session}
&
\ctrread(\key) \defeq &
\begin{session}
\begin{transaction}
\plookup{\pv{x}}{\key}
\end{transaction}
\end{session}
\end{array}
\]%
}%
%
Command \( \plookup{\pv{x}}{\key} \) reads the value of key \( \key \) to
local variable \( \vx \); command \( \pmutate{\key}{\pv{x}+1} \)
writes the value of \( \pv{x}+1 \) to key \( \key \).  The code of each
operation is wrapped in square brackets, denoting that 
%the enclosed code 
it must be executed \emph{atomically} as a transaction.  

Correctness of atomic transactions is subtle, depending heavily on the
particular consistency model under consideration.  
Consider the client program
$\prog_{\mathsf{LU}} = \left(\cl_1 : \ctrinc(\key) \;|| \; \cl_2:
  \ctrinc(\key) \right)$, where clients $\cl_1$ and $\cl_2$ run
$\ctrinc(\key)$ concurrently.
% below 
%%
%\begin{align}
%	\cl_1: \ctrinc(\key)
%	\;\; || \;\;
%	\cl_2: \ctrinc(\key)
%	\tag{\textsc{LU}}
%	\label{prog:inc2}
%\end{align}%
%
Let us assume that  $\key$ initially holds value $0$.
Intuitively, since transactions are executed atomically, after both
calls to $\ctrinc(\key)$ have terminated, the counter should hold 
the value $2$.
Indeed, this is the only outcome allowed under 
a well-known
consistency model called  \emph{serialisability}: transactions
appear to execute in a sequential (serial) order, one after another.
Implementing serialisability in distributed kv-stores comes at a
significant performance cost. Therefore, implementers are content with
{weaker} consistency models~\cite{.}. 

%that have been implemented both in
%replicated and partitioned databases
%\cite{ramp,rola,cops,wren,redblue,PSI,NMSI,gdur,clocksi,distrsi}.
When clients can execute $\ctrinc(\key)$ at different distributed
sites,
if the kv-store provides no synchronisation mechanism for transactions,
then it is possible for both clients to read the same initial value $0$ for $\key$ at their
distinct replicas, update them to $1$, and propagate their updates. Consequently, both
sites  are unchanged with value  $1$ for $\key$.
This weak behaviour is known as the \emph{lost update} anomaly, which
is  allowed under the consistency model called {\em causal consistency} \cite{cops,wren,redblue}.


%synchronisation among replicas ensures that they are in a consistent (albeit out-of-date) state.

\input{lu-kvstores.tex}
\mypar{Centralised Operational Semantics}

A well-known declarative approach for providing general reasoning
about clients of distributed kv-stores is to use  execution 
graphs~\cite{bothsorts}, where nodes are atomic transactions and edges describe the
known dependencies between transactions. The graphs capture the
behaviour of the whole program, with different consistency models
corresponding to different sets of axioms holding for the graphs. In
contrast, we provide an interleaving operational semantics based on
abstract centralised state. The centralised state comprises a
centralised, multi-versioned kv-store, which is {\em global} in the
sense that no update has been lost, and client views of the store,
which are {\em partial} in the sense that clients may see different 
subsets of the versions in the kv-store. Each update is given by either
a simple primitive command or an atomic transaction. The atomic
transaction steps are subject to an {\em execution test} which
analyses the state to determine whether the update is allowed by 
the associated  consistency model. 



Let us introduce  our global kv-stores and partial client views by
showing that we can reproduce the lost update anomaly given by 
$\prog_{\mathsf{LU}}$. 
Our kv-stores are functions mapping keys to lists of versions, where
the versions  record all the values written to each key together with the
meta-data of the transactions that access it. 
The initial kv-store comprises a single key $\key$, with only one
possible 
version $(0, \txid_{0}, \emptyset)$,  stating that $\key$ holds value $0$, 
that the version \emph{writer} is the initialising transaction
$\txid_0$ (this version was written by $\txid_0$), 
and that the version \emph{reader set} is empty (no transaction has read this version as of yet). 
\Cref{fig:counter_kv_initial} depicts this initial kv-store, with the version
represented as a box sub-divided in three sections: the version value $0$;
the writer $t_0$; and the reader set $\emptyset$. 




% implementation details of the distributed kv-store and, in
%particular, on its \emph{consistency model}.



First, suppose that $\cl_1$  invokes $\ctrinc$ on
\cref{fig:counter_kv_initial}. It does this by choosing a fresh
transaction identifier, $\txid$, 
and then proceeds with $\ctrinc(\key)$. It reads the initial version
of $\key$ with value $0$ 
and then writes a new value $1$ for $\key$. 
The resulting kv-store is depicted in \cref{fig:counter_kv_first_inc},
where  the initial version of $\key$  has been  updated to reflect that it
has been read by $\txid$. 

Second, client $\cl_2$ invokes $\ctrinc$ on
\cref{fig:counter_kv_first_inc}.  As there are now two versions
available for $\key$, we need to determine the version from which
$\cl_2$ fetches its value, before running $\ctrinc(\key)$.  This is
where \emph{client views} come into play.  Intuitively, a view of
client $\cl_2$ comprises those versions in the kv-store that are
\emph{visible} to $\cl_2$: that is, those whose values can be read by
$\cl_2$.  If more than one version is visible, then the newest
(right-most) version is selected, modelling the \emph{last-write-wins}
resolution policy used by many distributed
kv-stores~\cite{vogels:2009:ec:1435417.1435432}.  In our example,
there are two possible view candidates for $\cl_2$ when running
$\ctrinc(\key)$ on \cref{fig:counter_kv_first_inc}: (1) one containing
just the initial version of $\key$; (2) the other containing both
versions of $\key$.%
\footnote{ As we explain in \cref{sec:mkvs-view}, we always require
  the view of a client to have the initial version of each key marked
  as visible.}  For (1), the view is depicted in
\cref{fig:counter_kv_view}.  Client $\cl_2$ chooses a fresh
transaction identifier $t'$, reads the initial value $0$ and writes a
new version with value $1$, as depicted in
\cref{fig:counter_kv_final}.  Such a kv-store does not contain a
version with value $2$, despite two increments on $\key$, producing
the lost update anomaly.  For (2), client $cl_2$ reads the latest
value $1$ and writes a new version with value $2$.

To avoid undesirable behaviour, such as the lost update anomoly, we
use an {\em execution test} which restricts the possible update at the
point of the transaction commit.  One such test is to enforce a client
to commit a transaction writing to $\key$ if and only if its view
contains all versions available in the global state for $\key$.  This
prevents $\cl_2$ from running $\ctrinc(\key)$ on
\cref{fig:counter_kv_first_inc} if its view only contains the initial
version of $\key$.  Instead, the $\cl_2$ view must contain both
versions of $\key$, thus enforcing $\cl_2$ to write a version with
value $2$ after running $\ctrinc(\key)$. This particular test
corresponds to \emph{write-conflict-freedom} of distributed kv-stores:
at most one concurrent transaction can write to a key at any one time.
In Section~\ref{4}, we give many examples of execution tests and their
associated consistency models on kv-stores. In Section~\ref{.}, we
show that our operational definitions of consistency models are
equivalent to the declarative definitions of consistency models for 
execution graphs. 




%This property is known as \emph{write conflict freedom}. 




\mypar{COPS Protocol} The first application of our operational
semantics is to determine that implementations of distributed
kv-stores satisfy certain consistency models. We have shown that the
COPS protocol for implementing a replicated database satisfies {\em
  causal consistency},  and the Clock-SI protocol for implementing a
partitioned database satisfies the consistency model called {\em
  snapshot isolation}. We discuss the  COPS protocol  in this section. 



\pg{Now below needs to be summarised and made to fit the above.}

COPS is a fully replicated database where  each replica stores multiple versions of each key. 
%contains all keys.
In COPS, a version \( \ver \) on a replica contains a key, a value, a
time-stamp identifying the time when a client first requested the
replica to write the version, and a set of dependencies
$\depOf[\ver]$.  Each dependency consists of a key and a time-stamp of
the version of that key on which $\ver$ directly depends.  This
 time-stamp consists of the local, real time when a replica
first committed the version plus the replica identifier. COPS assumes
a total order among replica identifiers. Thus,  time-stamps can be
totally ordered lexicographically over pairs of real-time of
operations and replica identifiers.

The COPS API provides two operations for  clients: writing a single
key; and reading 
a set of keys atomically. Each instance of these operations is processed by a single replica. 
Each client maintains a \emph{context}, which is a set of dependencies
corresponding to the versions it observes.  
%future operations that the client performs depend from such versions.

When client $\cl$ requests to write key $\key$ to replica $r$, it
sends a message containing the value $\val$ to be written and its
context $ctx$ to $r$. It then waits for a reply from the replica. 
Upon receiving the message, $r$ produces a monotonically increasing time-stamp $t$, which it uses to install a new version $\ver = (\val, t, ctx)$ for the key $\key$. 
Note that the dependency set of $\ver$ is the context $ctx$ of the client.
%The version carries the value received, and the context received as its set of dependencies: 
It thus  depends on other versions previously observed by the client $\cl$. 
Replica $r$ then sends the time-stamp $t$ back to the client, which will incorporate the pair $(\key, t)$ in its local context: 
the client observes the write it performed. Finally, replica $r$ propagates the version written to other replicas asynchronously, 
using \emph{causal delivery}: when replica $r'$ receives a version $\ver$ from some replica $\ver$, it 
will wait to commit all its dependencies to its local state, before committing $\ver$ itself.
%This property ensures that 
The set of versions contained in the local state of each replica is closed with respect to causal dependencies.
%A COPS client interacts with a single replica at a time via COPS API,
%either single-write or multiple-read transactions.
%Replicas \( \repl \) store all versions that has been received from clients, or other replicas via synchronisation.
%Each version \( \ver \) includes a key, a value, a unique timestamp 
%assigned by the replica who initially received this version from a client,
%and a \emph{dependency set}, written by \( \depOf[\ver]\).
%This dependency set traces the versions on which $\ver$ causally depends.
%A synchronisation message with a version \( \ver \) will be received only 
%when all the versions in the dependency set have been received.
%This guarantees that for any version \( \ver \) in a replica \( \repl \), written by \( \ver \in \repl \),
%the versions on which \( \ver \) depends also are in the replica, \( \depOf[\ver] \subseteq \repl \).
%COPS assumes that all versions eventually deliver to all replicas.
%Replicas themselves are assigned with unique identifiers that are totally ordered.
%As such, all versions on all keys are totally ordered,
%which, together with eventual delivery, guarantees eventual consistency among all replicas.
%The COPS API follows the multiple-readers-single-writer paradigm: 
%at any given time, the database can be accessed by either multiple read-only concurrent transactions, or single writing transactions. 
%Each version $\ver$ records a value and a \emph{dependency set}, written $\depOf[\ver]$,
%tracking the versions on which $\ver$ causally depends.
%This causal dependency contains previous reads and writes from the same client, and their dependencies,
%that is, 
%\( \left( \left( \SO \cup \WR \right) ^{-1}\right)^{*}(\wtOf[\ver]) \subseteq \depOf[\ver] \).
%
%For a client to commit a single-write transaction, hence a new value \( \val \) of a key \( \key \),
%it sends the new value \( \val \) and more importantly the dependency set \( D \), to a replica
%in which the \( \key, \val, D \) together with a newly assigned timestamp \( \txid \), turn into a new version \( \ver \);
%then the new timestamp returns to the client.
%To construct the dependency set for the new version,
%each client maintains a \emph{client context} that tracks the versions that have been either fetched from, or committed to any replicas. 
%For a single-write transaction, the client context becomes the dependency set \( D \) of the new version \( \ver \) which ensures \( D \) contains all versions that \( \ver \) causally depends on.

When a client requests to read a set of keys \( \{\key_1, \dots,
\key_n\} \) from a  replica $r$, it sends a message 
containing the set of keys to the replica and  waits for a reply from $r$. Upon receiving the message, replica $r$ builds a causally consistent snapshot, i.e. 
a mapping from the set $\{\key_{1},\cdots, \key_{n}\}$ to values, in two rounds. 
First, $r$ fetches from its state 
%A client can read some keys \( \key_1, \dots, \key_n \) in a multiple-read transaction through requesting a replica; 
%the replica fetches 
the most recent version \( \ver_{\key_i} \) for each $i
=0,\cdots,n$. Each of these operations happens atomically. However, 
new versions may be committed at $r$ in-between the times the replica fetches the versions for the keys 
$\key_{i}$, for $0 < i < n$,  and $\key_{n}$. In particular, it may be the case that the version $\ver_{\key_{n}}$ causally depends 
on a version $\ver'_{\key_{i}}$ that is newer than $\ver_{\key_{i}}$,  where $0 < i < n$, but commits at replica $r$ 
after the version $\ver_{\key_{i}}$ is read. The set of versions read by $r$ is not causally consistent. 
To overcome this problem, the replica $r$ uses the time-stamp $t_{n}$ of the version $\ver_{\key_{n}}$ as an upper-bound, 
and it proceeds to fetch the most up-to-date version of each key $\key_{1}, \cdots, \key_{n}$ with time-stamp 
at most $t_{n}$. At the time $\ver_{\key_{n}}$ is read,  all its causal dependencies must be included in the local 
state of $r$, hence the new snapshot built by $r$ is causally consistent. The snapshot is sent  from $r$ 
to the client, together with the set of dependencies of each version
read which are 
included into the local context of the client.
% Future operations of
%the client depend frthe versions 
%the client has read.
\ac{I will leave this to myself for future reference: why the hell does COPS do this in two rounds? It looks like you can do 
this in a single round, by simply taking the time-stamp of the first version you read as an upper bound for future reads.
Update: Went to check directly on the COPS protocol, and indeed COPS really does something more - but not much more - complicated. 
The main aspect from which COPS differs from Shale's idealisation of it is that (1) reads happen concurrently, and the 
one with highest time-stamp is used to fix the bound on versions in the first round, and (2) in the second round not all keys 
are re-fetched, but only those that are really needed to recover a causally consistent snapshot.}


\mypar{Generael Robustness Results} 
The second application of our operational semantics is to prove
general robustness results for clients with respect to specific
consistency models. ....

\pg{now what are we going to say here,
I suggest a summary of results.........}



\pg{*******don't know where to put all this, if anywhere. Ithink this
  is for a later section not section 2.} 


.........probably note here...........In \cref{sec:program-analysis}, we prove that if the kv-store 
ensures write-conflict freedom as well as few other properties, then clients can increment 
and read from a single counter as if the kv-store were (strictly) serialisable.

However, the situation becomes more complicated when the kv-store contains multiple counters:  
since each client has its own view on the kv-store, and views of clients are independent from each other, it is possible for two 
clients to observe the increments on two distinct counters, $\Counter(\key_1)$ and $\Counter(\key_2)$, in different order. 
For instance, consider the following program:

\vspace{-5pt}
{%
\displaymathfont
\begin{align}
		\cl_0: 
		 \ctrinc(\key_1) ; \ctrinc(\key_2)
		 \;\; || \;\;  \cl_1: 
		 \ctrread(\key_1) ; \ctrread(\key_2)
		  \;\; || \;\;  \cl_2: 
		 \ctrread(\key_1); \ctrread(\key_2)
	\tag{\textsc{LF}}
	\label{prog:LF}
\end{align}	 
}%
%
%\begin{align}
%	\begin{array}[t]{@{} r @{\hspace{2pt}} l || r @{\hspace{2pt}} l || r @{\hspace{2pt}} l @{}}
%		\cl_0: 
%		& \ctrinc(\key_1) ; \ctrinc(\key_2)
%		& \;\; || \;\; & \cl_1: 
%		& \ctrread(\key_1) ; \ctrread(\key_2)
%		&  \;\; || \;\; & \cl_2: 
%		& \ctrread(\key_1); \ctrread(\key_2)\\
%%
%%		& \ctrinc(\key_2) 
%%		&& \ctrread(\key_2)
%%		&& \ctrread(\key_2)
%	\end{array}
%	\tag{\textsc{LF}}
%	\label{prog:LF}
%\end{align}
%   
Suppose that $\cl_0$ executes first by incrementing $\key_1$, $\key_2$: 
this results in $\key_1$ and $\key_2$ having two versions with values $0$ and $1$ each. 
Client $\cl_1$ executes its transactions next, using a view that 
%Let us assume that the view of $\cl_1$ 
contains both versions of $\key_1$, but only 
the initial version of $\key_2$:  $\cl_1$ then reads $1$ for $\key_1$ and $0$ for $\key_2$, 
or equivalently it observes
%When next client $\cl_1$ executes, it thus reads $1$ for $\key_1$ and $0$ for $\key_2$; 
%that is, 
%from the point of view of $\cl_1$, 
the increment of $\key_1$ 
happening before the increment of $\key_2$. 
Finally, $\cl_2$ executes its transactions using a view that contains both versions for $\key_2$, but only 
the initial version of $\key_1$: 
$\cl_2$ reads $0$ for $\key_1$ and $1$ for $\key_2$, that is it
%that is, from the point of view of $\cl_2$, 
observes the increment of $\key_2$ 
happening before the increment of $\key_1$. 
This behaviour is known as the \emph{long fork} anomaly (\cref{fig:cp-disallowed}). 

The long fork anomaly is disallowed under strong models, \eg \(\SER\) and 
\(\SI\), 
but is allowed under weaker models \eg \(\PSI\) and \(\CC\). 
To capture such consistency models and rule out the long form anomaly as a possible result 
of the program \eqref{prog:LF}, we must strengthen the execution test associated with the kv-store. 
For \(\SER\), we strengthen the execution test by requiring that a client can execute a transaction 
only if its view contains all the versions available in the global state. 
For SI, 
the candidate execution test recovers the order in which 
updates of versions have been observed by different clients (\eg $\cl_1$), 
and allows a transaction to commit only if the observations made by the committing client (\eg $\cl_2$) 
are consistent with previous clients (\ie $\cl_1$): we give the formal definition of this execution test 
in \cref{sec:cm}.
Under such strengthened execution tests, e.g. the one for \(\SI\), in the \eqref{prog:LF} example $\cl_2$ cannot
observe $1$ for $\key_2$ after observing $0$ for $\key_1$; 
this is because $\cl_1$ has already established that the increment on $\key_2$ happens after 
the one of $\key_1$. 
In \cref{sec:program-analysis}, we prove that if the kv-store consists of multiple counter objects, and the execution test employed by transactions guarantees \( \SI \), then the kv-store 
behaves as is it were (strictly) serialisable.
\sx{
    In fact, with our new operational semantics, we spot a new consistency model that is slightly weaker than \( \SI \).
    We call it weak snapshot isolation \( \WSI \).
    This model also guarantee the kv-stores of multi-counters only have serialisable behaviours.
}
As we demonstrate in \cref{sec:cm}, using execution tests on kv-stores, we can define all well-known consistency models (weak or strong) subject to a few basic conditions. 
\sx{we should again mention atomic visibility which is now called snapshot property here:
One of the conditions is snapshot property.
It means that, from the point view of clients a transaction read from an atomic snapshot took at the beginning.
This is different from the implementation strategies.
For example, COPS \cite{cops}, a geo-replicated protocol for causal consistency, tracks the history states and dependencies between them to ensure that 
a transaction receives values that can be view as it come from an atomic snapshot.
Clock-SI \cite{clocksi}, a protocol for \(\SI\) under partitioned key-value stores, 
achieves snapshot property via matching local time of partitions against the start time of a transaction.
}
Moreover, in \cref{sec:verify-impl} we encode two different distributed protocols using kv-stores and views: COPS \cite{cops}, 
a geo-replicated protocol for causal consistency, and Clock-SI \cite{clocksi}, a protocol for \(\SI\) under partitioned key-value stores.
Each of these protocols is verified against our definition using execution tests.
