\newcommand{\Counter}{\ensuremath{\mathsf{Counter}}}
\newcommand{\ctrinc}{\ensuremath{\mathsf{inc}}}
\newcommand{\ctrread}{\ensuremath{\mathsf{read}}}
%
\section{Overview}
\label{sec:overview}


%synchronisation among replicas ensures that they are in a consistent (albeit out-of-date) state.
\mypar{COPS}
COPS is a fully replicated database where  each replica stores multiple versions of each key. 
%contains all keys.
In COPS, a version \( \ver \) on a replica contains a key, a value, a
time-stamp identifying the time when a client first requested the
replica to write the version, and a set of dependencies
$\depOf[\ver]$.  Each dependency consists of a key and a time-stamp of
the version of that key on which $\ver$ directly depends.  This
 time-stamp consists of the local, real time when a replica
first committed the version plus the replica identifier. COPS assumes
a total order among replica identifiers. Thus,  time-stamps can be
totally ordered lexicographically over pairs of real-time of
operations and replica identifiers.

The COPS API provides two operations for  clients: writing a single
key; and reading 
a set of keys atomically. Each instance of these operations is processed by a single replica. 
Each client maintains a \emph{context}, which is a set of dependencies
corresponding to the versions it observes.  
%future operations that the client performs depend from such versions.

When client $\cl$ requests to write key $\key$ to replica $r$, it
sends a message containing the value $\val$ to be written and its
context $ctx$ to $r$. It then waits for a reply from the replica. 
Upon receiving the message, $r$ produces a monotonically increasing time-stamp $t$, which it uses to install a new version $\ver = (\val, t, ctx)$ for the key $\key$. 
Note that the dependency set of $\ver$ is the context $ctx$ of the client.
%The version carries the value received, and the context received as its set of dependencies: 
It thus  depends on other versions previously observed by the client $\cl$. 
Replica $r$ then sends the time-stamp $t$ back to the client, which will incorporate the pair $(\key, t)$ in its local context: 
the client observes the write it performed. Finally, replica $r$ propagates the version written to other replicas asynchronously, 
using \emph{causal delivery}: when replica $r'$ receives a version $\ver$ from some replica $\ver$, it 
will wait to commit all its dependencies to its local state, before committing $\ver$ itself.
%This property ensures that 
The set of versions contained in the local state of each replica is closed with respect to causal dependencies.
%A COPS client interacts with a single replica at a time via COPS API,
%either single-write or multiple-read transactions.
%Replicas \( \repl \) store all versions that has been received from clients, or other replicas via synchronisation.
%Each version \( \ver \) includes a key, a value, a unique timestamp 
%assigned by the replica who initially received this version from a client,
%and a \emph{dependency set}, written by \( \depOf[\ver]\).
%This dependency set traces the versions on which $\ver$ causally depends.
%A synchronisation message with a version \( \ver \) will be received only 
%when all the versions in the dependency set have been received.
%This guarantees that for any version \( \ver \) in a replica \( \repl \), written by \( \ver \in \repl \),
%the versions on which \( \ver \) depends also are in the replica, \( \depOf[\ver] \subseteq \repl \).
%COPS assumes that all versions eventually deliver to all replicas.
%Replicas themselves are assigned with unique identifiers that are totally ordered.
%As such, all versions on all keys are totally ordered,
%which, together with eventual delivery, guarantees eventual consistency among all replicas.
%The COPS API follows the multiple-readers-single-writer paradigm: 
%at any given time, the database can be accessed by either multiple read-only concurrent transactions, or single writing transactions. 
%Each version $\ver$ records a value and a \emph{dependency set}, written $\depOf[\ver]$,
%tracking the versions on which $\ver$ causally depends.
%This causal dependency contains previous reads and writes from the same client, and their dependencies,
%that is, 
%\( \left( \left( \SO \cup \WR \right) ^{-1}\right)^{*}(\wtOf[\ver]) \subseteq \depOf[\ver] \).
%
%For a client to commit a single-write transaction, hence a new value \( \val \) of a key \( \key \),
%it sends the new value \( \val \) and more importantly the dependency set \( D \), to a replica
%in which the \( \key, \val, D \) together with a newly assigned timestamp \( \txid \), turn into a new version \( \ver \);
%then the new timestamp returns to the client.
%To construct the dependency set for the new version,
%each client maintains a \emph{client context} that tracks the versions that have been either fetched from, or committed to any replicas. 
%For a single-write transaction, the client context becomes the dependency set \( D \) of the new version \( \ver \) which ensures \( D \) contains all versions that \( \ver \) causally depends on.

When a client requests to read a set of keys \( \{\key_1, \dots,
\key_n\} \) from a  replica $r$, it sends a message 
containing the set of keys to the replica and  waits for a reply from $r$. Upon receiving the message, replica $r$ builds a causally consistent snapshot, i.e. 
a mapping from the set $\{\key_{1},\cdots, \key_{n}\}$ to values, in two rounds. 
First, $r$ fetches from its state 
%A client can read some keys \( \key_1, \dots, \key_n \) in a multiple-read transaction through requesting a replica; 
%the replica fetches 
the most recent version \( \ver_{\key_i} \) for each $i
=0,\cdots,n$. Each of these operations happens atomically. However, 
new versions may be committed at $r$ in-between the times the replica fetches the versions for the keys 
$\key_{i}$, for $0 < i < n$,  and $\key_{n}$. In particular, it may be the case that the version $\ver_{\key_{n}}$ causally depends 
on a version $\ver'_{\key_{i}}$ that is newer than $\ver_{\key_{i}}$,  where $0 < i < n$, but commits at replica $r$ 
after the version $\ver_{\key_{i}}$ is read. The set of versions read by $r$ is not causally consistent. 
To overcome this problem, the replica $r$ uses the time-stamp $t_{n}$ of the version $\ver_{\key_{n}}$ as an upper-bound, 
and it proceeds to fetch the most up-to-date version of each key $\key_{1}, \cdots, \key_{n}$ with time-stamp 
at most $t_{n}$. At the time $\ver_{\key_{n}}$ is read,  all its causal dependencies must be included in the local 
state of $r$, hence the new snapshot built by $r$ is causally consistent. The snapshot is sent  from $r$ 
to the client, together with the set of dependencies of each version
read which are 
included into the local context of the client.
% Future operations of
%the client depend frthe versions 
%the client has read.
\ac{I will leave this to myself for future reference: why the hell does COPS do this in two rounds? It looks like you can do 
this in a single round, by simply taking the time-stamp of the first version you read as an upper bound for future reads.
Update: Went to check directly on the COPS protocol, and indeed COPS really does something more - but not much more - complicated. 
The main aspect from which COPS differs from Shale's idealisation of it is that (1) reads happen concurrently, and the 
one with highest time-stamp is used to fix the bound on versions in the first round, and (2) in the second round not all keys 
are re-fetched, but only those that are really needed to recover a causally consistent snapshot.}



Consider a simple counter object, $\mathsf{Counter}(\key)$, 
defined over a distributed kv-store.
Clients can manipulate the value of key $\key$ via two operations, 
$\ctrinc(\key)$ and $\ctrread(\key)$:

\vspace{-5pt}
{%
\displaymathfont
\[%
\begin{array}{r @{\hspace{2pt}} l @{\hspace{18pt}} r @{\hspace{2pt}} l}
\ctrinc(\key) \defeq 
&
\begin{session}
\begin{transaction}
\plookup{\pv{x}}{\key}; \ 
\pmutate{\key}{\pv{x}+1}
\end{transaction}
\end{session}
&
\ctrread(\key) \defeq &
\begin{session}
\begin{transaction}
\plookup{\pv{x}}{\key}
\end{transaction}
\end{session}
\end{array}
\]%
}%
%
Command \( \plookup{\pv{x}}{\key} \) reads the value of key \( \key \) to
local variable \( \vx \); command \( \pmutate{\key}{\pv{x}+1} \)
writes the value of \( \pv{x}+1 \) to key \( \key \).  The code of each
operation is wrapped in square brackets, denoting that 
%the enclosed code 
it must be executed \emph{atomically} as a transaction.  
Correctness of atomic transactions is subtle, depending heavily
on implementation details of the distributed kv-store and, in
particular, on its \emph{consistency model}.


\mypar{Consistency Models}
A well-known consistency model is that of \emph{serialisability}: transactions appear to execute in a sequential (serial) order,
one after another. 
Implementing serialisability in distributed kv-stores
comes at a significant performance cost. Therefore, implementers are content
with \emph{weaker} consistency models, 
that have been implemented both in replicated and partitioned databases 
\cite{ramp,rola,cops,wren,redblue,PSI,NMSI,gdur,clocksi,distrsi}.

We motivate these weak consistency models using replicated kv-stores: 
%In
%such stores, 
clients run each operation on an arbitrary replica, and
propagate the effects of the operations (if any) to other
replicas. In this setting, concurrent calls to 
operations can lead to weak behaviours not present with
serialisability.
%For instance, 
consider the program $\prog_{\mathsf{LU}} = \left(\cl_1 : \ctrinc(\key) \;|| \; \cl_2: \ctrinc(\key) \right)$, where clients $\cl_1$ and 
$\cl_2$  run $\ctrinc(\key)$ concurrently.
% below 
%%
%\begin{align}
%	\cl_1: \ctrinc(\key)
%	\;\; || \;\;
%	\cl_2: \ctrinc(\key)
%	\tag{\textsc{LU}}
%	\label{prog:inc2}
%\end{align}%
%
Let us assume that  $\key$ initially holds value $0$.
Intuitively, since transactions are executed atomically, after both
calls to $\ctrinc(\key)$ have terminated, the counter should hold 
the value $2$.
Indeed, this is the only outcome allowed under serialisability. 
However, when clients execute $\ctrinc(\key)$ at different replicas,
if the kv-store provides no synchronisation mechanism for transactions,
then it is possible for both clients to read the same initial value $0$ for $\key$ at their
distinct replicas, update them to $1$, and propagate their updates. Consequently, both
replicas are unchanged with value  $1$ for $\key$.
This weak behaviour is known as the \emph{lost update} anomaly, which
is  allowed under causal consistency \cite{cops,wren,redblue}.

\input{lu-kvstores.tex}
\mypar{Centralised KV-Stores and Views}
Reasoning about programs in a distributed database by capturing the 
whole system state may be cumbersome and error-prone, 
due to the large amount of information that needs to keep tracked. 
Declarative formalisms overcome this issue by abstracting 
from the system state, and relying on the history of operations performed 
by transactions instead.

Here we take a different approach, and abstract from 
distribution by projecting the local state of each component 
of the kv-store into a global, \emph{multi-versioned} centralised state: 
we record all versions of each key written, 
together with the meta-data of the transactions that access it. 
Clients use a mechanism called \emph{client views} to determine 
the subset of versions in the kv-store that are made available to them:
this lets clients observe different states 
of the kv-store, allowing to capture weak behaviours such as the \emph{lost update} anomaly 
while retaining the atomic execution of transactions.


Let us illustrate how we can produce the lost update anomaly in $\prog_{\mathsf{LU}}$ using our kv-stores. 
The initial kv-store comprises a single key $\key$, with only one 
version, $(0, \txid_{0}, \emptyset)$, stating that $\key$ holds value $0$, 
that the version \emph{writer} is the initialising transaction $\txid_0$ (this version was written by $\txid_0$), 
and that the version \emph{reader set} is empty (no transaction has read this version as of yet). 
\Cref{fig:counter_kv_initial} depicts this initial kv-store, with the version
represented as a box sub-divided in three sections;
proceeding clockwise from the left, they represent the version value, writer and reader set, respectively.

Suppose that $\cl_1$ first invokes $\ctrinc$ on \cref{fig:counter_kv_initial}. 
In order to mark the versions that $cl_1$ reads and writes while executing the underlying transaction of $\ctrinc$,  
$\cl_1$ first obtains a unique transaction identifier, $\txid$, 
and then proceeds with $\ctrinc(\key)$. 
It then reads the (only) version of $\key$ (with value $0$), 
and writes a new value $1$ for $\key$. 
The resulting kv-store is depicted in \cref{fig:counter_kv_first_inc}.
Note that the initial version of $\key$ is updated to reflect that it has been read by $\txid$; 
moreover, versions are ordered (left to right), from the oldest to the newest.

Next, client $\cl_2$ invokes $\ctrinc$ on \cref{fig:counter_kv_first_inc}. 
As there are now two versions available for $\key$, 
we need to determine the version from which $\cl_2$ fetches its value, before running $\ctrinc(\key)$.
This is where \emph{client views} come into play.
Intuitively, a view of client $\cl_2$ comprises those versions in the kv-store that are \emph{visible} to $\cl_2$, 
\ie those whose values can be read by $\cl_2$. 
If more than one version is visible, then the newest (right-most) such version is selected, 
modelling the \emph{last write wins} resolution policy used by several kv-stores~\cite{vogels:2009:ec:1435417.1435432}. 
In our example, there are two possible view candidates for $\cl_2$ when running $\ctrinc(\key)$ on \cref{fig:counter_kv_first_inc}: 
one containing only the initial version of $\key$, 
another containing both versions of $\key$.%
\footnote{
As we explain in \cref{sec:mkvs-view}, we always require the view of a client 
to have the initial version of each key marked as visible.}
In the former case, the view is depicted in \cref{fig:counter_kv_view}:
running $\ctrinc(\key)$ on this view reads $0$ and writes a new version with value $1$, as depicted in \cref{fig:counter_kv_final}.
Such a kv-store does not contain a version with value $2$, despite two increments on $\key$, producing the lost update anomaly.
In the latter case, running $\ctrinc(\key)$ reads the latest value $1$ and writes a new version with value $2$.

\mypar{Execution Tests}
To avoid the lost update anomaly, distributed kv-stores introduce a check at commit time that enforces 
\emph{write conflict freedom}: at most one among concurrent transactions writing to the same key commits. 
%This property is known as \emph{write conflict freedom}. 
In our framework, we can simulate this behaviour by introducing the notion of \emph{execution tests}. 
Intuitively, an execution test determines whether a client with a given view can execute a transaction. 
For example, write-conflict freedom can be enforced by requiring that a client commit a transaction writing to $\key$ 
only if its view contains all versions available in the global state for such a view. 
In executions of $\prog_{\mathsf{LU}}$,
% \eqref{prog:inc2} example, 
this prevents $\cl_2$ from running $\ctrinc(\key)$ on \cref{fig:counter_kv_first_inc}
if its view only contains the initial version of $\key$. 
Instead, the $\cl_2$ view must contain both versions of $\key$, 
thus enforcing $\cl_2$ to write a version with value $2$ after running $\ctrinc(\key)$
In \cref{sec:program-analysis}, we prove that if the kv-store 
ensures write-conflict freedom as well as few other properties, then clients can increment 
and read from a single counter as if the kv-store were (strictly) serialisable.

However, the situation becomes more complicated when the kv-store contains multiple counters:  
since each client has its own view on the kv-store, and views of clients are independent from each other, it is possible for two 
clients to observe the increments on two distinct counters, $\Counter(\key_1)$ and $\Counter(\key_2)$, in different order. 
For instance, consider the following program:

\vspace{-5pt}
{%
\displaymathfont
\begin{align}
		\cl_0: 
		 \ctrinc(\key_1) ; \ctrinc(\key_2)
		 \;\; || \;\;  \cl_1: 
		 \ctrread(\key_1) ; \ctrread(\key_2)
		  \;\; || \;\;  \cl_2: 
		 \ctrread(\key_1); \ctrread(\key_2)
	\tag{\textsc{LF}}
	\label{prog:LF}
\end{align}	 
}%
%
%\begin{align}
%	\begin{array}[t]{@{} r @{\hspace{2pt}} l || r @{\hspace{2pt}} l || r @{\hspace{2pt}} l @{}}
%		\cl_0: 
%		& \ctrinc(\key_1) ; \ctrinc(\key_2)
%		& \;\; || \;\; & \cl_1: 
%		& \ctrread(\key_1) ; \ctrread(\key_2)
%		&  \;\; || \;\; & \cl_2: 
%		& \ctrread(\key_1); \ctrread(\key_2)\\
%%
%%		& \ctrinc(\key_2) 
%%		&& \ctrread(\key_2)
%%		&& \ctrread(\key_2)
%	\end{array}
%	\tag{\textsc{LF}}
%	\label{prog:LF}
%\end{align}
%   
Suppose that $\cl_0$ executes first by incrementing $\key_1$, $\key_2$: 
this results in $\key_1$ and $\key_2$ having two versions with values $0$ and $1$ each. 
Client $\cl_1$ executes its transactions next, using a view that 
%Let us assume that the view of $\cl_1$ 
contains both versions of $\key_1$, but only 
the initial version of $\key_2$:  $\cl_1$ then reads $1$ for $\key_1$ and $0$ for $\key_2$, 
or equivalently it observes
%When next client $\cl_1$ executes, it thus reads $1$ for $\key_1$ and $0$ for $\key_2$; 
%that is, 
%from the point of view of $\cl_1$, 
the increment of $\key_1$ 
happening before the increment of $\key_2$. 
Finally, $\cl_2$ executes its transactions using a view that contains both versions for $\key_2$, but only 
the initial version of $\key_1$: 
$\cl_2$ reads $0$ for $\key_1$ and $1$ for $\key_2$, that is it
%that is, from the point of view of $\cl_2$, 
observes the increment of $\key_2$ 
happening before the increment of $\key_1$. 
This behaviour is known as the \emph{long fork} anomaly (\cref{fig:cp-disallowed}). 

The long fork anomaly is disallowed under strong models, \eg \(\SER\) and 
\(\SI\), 
but is allowed under weaker models \eg \(\PSI\) and \(\CC\). 
To capture such consistency models and rule out the long form anomaly as a possible result 
of the program \eqref{prog:LF}, we must strengthen the execution test associated with the kv-store. 
For \(\SER\), we strengthen the execution test by requiring that a client can execute a transaction 
only if its view contains all the versions available in the global state. 
For SI, 
the candidate execution test recovers the order in which 
updates of versions have been observed by different clients (\eg $\cl_1$), 
and allows a transaction to commit only if the observations made by the committing client (\eg $\cl_2$) 
are consistent with previous clients (\ie $\cl_1$): we give the formal definition of this execution test 
in \cref{sec:cm}.
Under such strengthened execution tests, e.g. the one for \(\SI\), in the \eqref{prog:LF} example $\cl_2$ cannot
observe $1$ for $\key_2$ after observing $0$ for $\key_1$; 
this is because $\cl_1$ has already established that the increment on $\key_2$ happens after 
the one of $\key_1$. 
In \cref{sec:program-analysis}, we prove that if the kv-store consists of multiple counter objects, and the execution test employed by transactions guarantees \( \SI \), then the kv-store 
behaves as is it were (strictly) serialisable.
\sx{
    In fact, with our new operational semantics, we spot a new consistency model that is slightly weaker than \( \SI \).
    We call it weak snapshot isolation \( \WSI \).
    This model also guarantee the kv-stores of multi-counters only have serialisable behaviours.
}
As we demonstrate in \cref{sec:cm}, using execution tests on kv-stores, we can define all well-known consistency models (weak or strong) subject to a few basic conditions. 
\sx{we should again mention atomic visibility which is now called snapshot property here:
One of the conditions is snapshot property.
It means that, from the point view of clients a transaction read from an atomic snapshot took at the beginning.
This is different from the implementation strategies.
For example, COPS \cite{cops}, a geo-replicated protocol for causal consistency, tracks the history states and dependencies between them to ensure that 
a transaction receives values that can be view as it come from an atomic snapshot.
Clock-SI \cite{clocksi}, a protocol for \(\SI\) under partitioned key-value stores, 
achieves snapshot property via matching local time of partitions against the start time of a transaction.
}
Moreover, in \cref{sec:verify-impl} we encode two different distributed protocols using kv-stores and views: COPS \cite{cops}, 
a geo-replicated protocol for causal consistency, and Clock-SI \cite{clocksi}, a protocol for \(\SI\) under partitioned key-value stores.
Each of these protocols is verified against our definition using execution tests.
