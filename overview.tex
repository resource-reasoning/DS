\section{Overview}
\label{sec:overview}

We motivate our key ideas, our centralised kv-stores, partial client views and execution tests,
using a $\mathsf{Counter}(\key)$ example.
We argue that our interleaving semantics is 
a good middle point to prove invariant properties such as robustness, and 
verify distributed protocols.

\mypar{Example} We use a simple $\mathsf{Counter}(\key)$ example to
 introduce our operational semantics.  Clients can manipulate the
value of key $\key$ via two transactions:

\vspace{-5pt}
{%
\displaymathfont
\[%
\begin{array}{r @{\hspace{2pt}} l @{\hspace{18pt}} r @{\hspace{2pt}} l}
\ctrinc(\key) \defeq 
&
\begin{session}
\begin{transaction}
\plookup{\pv{x}}{\key}; \ 
\pmutate{\key}{\pv{x}+1}
\end{transaction}
\end{session}
&
\ctrread(\key) \defeq &
\begin{session}
\begin{transaction}
\plookup{\pv{x}}{\key}
\end{transaction}
\end{session}
\end{array}
\]%
}%
%
Command \( \plookup{\pv{x}}{\key} \) reads the value of key \( \key \) to
local variable \( \vx \); command \( \pmutate{\key}{\pv{x}+1} \)
writes the value of \( \pv{x}+1 \) to key \( \key \).  The code of each
operation is wrapped in square brackets, denoting that 
it must be executed \emph{atomically} as a transaction.  

Consider a replicated database where
a client only interacts with one replica.
For such a database, the 
correctness of atomic transactions is subtle, depending heavily on the
particular consistency model under consideration.  
Consider the client program
$\prog_{\mathsf{LU}} = \left(\cl_1 : \ctrinc(\key) \;|| \; \cl_2:
  \ctrinc(\key) \right)$, 
where we assume that the clients \( \cl_1 \) and \( \cl_2 \) work on different replicas and
the $\key$ initially holds value $0$ in all replicas.
Intuitively, since transactions are executed atomically, after both
calls to $\ctrinc(\key)$ have terminated, the counter should hold 
the value $2$.
Indeed, this is the only outcome allowed under 
\SER\, where transactions
appear to execute in a sequential (serial) order, one after another.
The implementation of  \SER\ in distributed kv-stores comes at a
significant performance cost. Therefore, implementers are content with
{weaker} consistency models~\cite{ramp,rola,cops,wren,redblue,PSI,NMSI,gdur,clocksi,distrsi}. 
%that have been implemented both in
%replicated and partitioned databases
%\cite{ramp,rola,cops,wren,redblue,PSI,NMSI,gdur,clocksi,distrsi}.
For example, if the replicas provide no synchronisation mechanism for transactions,
then it is possible for both clients to read the same initial value $0$ for $\key$ at their
distinct replicas, update them to $1$, and eventually propagate their updates to other replicas. 
Consequently, both
sites  are unchanged with value  $1$ for $\key$.
This weak behaviour is known as the \emph{lost update} anomaly, which
is  allowed under the consistency model called {\em causal consistency} \cite{cops,wren,redblue}.

\input{lu-kvstores.tex}
\mypar{Centralised Operational Semantics}

A well-known declarative approach for providing general reasoning
about clients of distributed kv-stores is to use  execution 
graphs~\cite{adya-icde,adya,framework-concur,ev_transactions},
where nodes are atomic transactions and edges describe the
known dependencies between transactions. The graphs capture the
behaviour of the whole program, with different consistency models
corresponding to different sets of axioms constraining the graphs. 
However, execution graphs provide little information about how the 
state of a kv-store evolves throughout the execution of a program.
By contrast, we provide an interleaving operational semantics based on an
abstract centralised state. The centralised state comprises a
centralised, multi-versioned kv-store, which is {\em global} in the
sense that it contains all the versions written by clients, and client views of the store,
which are {\em partial} in the sense that clients may see different 
subsets of the versions in the kv-store. Each update is given by either
a simple primitive command or an atomic transaction. The atomic
transaction steps are subject to an {\em execution test} which
analyses the state to determine whether the update is allowed by 
the associated  consistency model. 



Let us introduce  our global kv-stores and partial client views by
showing that we can reproduce the lost update anomaly given by 
$\prog_{\mathsf{LU}}$.
Our kv-stores are functions mapping keys to lists of versions, where
the versions  record all the values written to each key together with the
meta-data of the transactions that access it. 
In the $\prog_{\mathsf{LU}}$ example, the initial kv-store comprises a single key $\key$, with only one possible 
version $(0, \txid_{0}, \emptyset)$,  stating that $\key$ holds value $0$, 
that the version \emph{writer} is the initialising transaction
$\txid_0$ (this version was written by $\txid_0$), 
and that the version \emph{reader set} is empty (no transaction has read this version as of yet). 
\Cref{fig:counter_kv_initial} depicts this initial kv-store, with the version
represented as a box sub-divided in three sections: the value $0$;
the writer $t_0$; and the reader set $\emptyset$. 




% implementation details of the distributed kv-store and, in
%particular, on its \emph{consistency model}.



First, suppose that $\cl_1$  invokes $\ctrinc$ on
\cref{fig:counter_kv_initial}. It does this by choosing a fresh
transaction identifier, $\txid$, 
and then proceeds with $\ctrinc(\key)$. It reads the initial version
of $\key$ with value $0$ 
and then writes a new value $1$ for $\key$. 
The resulting kv-store is depicted in \cref{fig:counter_kv_first_inc},
where  the initial version of $\key$  has been  updated to reflect that it
has been read by $\txid$. 

Second, client $\cl_2$ invokes $\ctrinc$ on
\cref{fig:counter_kv_first_inc}.  As there are now two versions
available for $\key$, we need to determine the version from which
$\cl_2$ fetches its value, before running $\ctrinc(\key)$.  This is
where \emph{client views} come into play.  Intuitively, a view of
client $\cl_2$ comprises those versions in the kv-store that are
\emph{visible} to $\cl_2$: that is, those whose values can be read by
$\cl_2$.  If more than one version is visible, then the newest
(right-most) version is selected, modelling the \emph{last-writer-wins}
resolution policy used by many distributed
kv-stores~\cite{vogels:2009:ec:1435417.1435432}.  In our example,
there are two possible view candidates for $\cl_2$ when running
$\ctrinc(\key)$ on \cref{fig:counter_kv_first_inc}: (1) one containing
just the initial version of $\key$; (2) the other containing both
versions of $\key$.%
\footnote{ As we explain in \cref{sec:mkvs-view}, we always require
  the view of a client to include the initial version of each key.}  
For (1), the view is depicted in
\cref{fig:counter_kv_view}.  Client $\cl_2$ chooses a fresh
transaction identifier $t'$, reads the initial value $0$ and writes a
new version with value $1$, as depicted in
\cref{fig:counter_kv_final}.  Such a kv-store does not contain a
version with value $2$, despite two increments on $\key$, producing
the lost update anomaly.  For (2), client $cl_2$ reads the newest
value $1$ and writes a new version with value $2$.

To avoid undesirable behaviour, such as the lost update anomaly, we
use an {\em execution test} which restricts the possible update at the
point of the transaction commit.  One such test is to enforce a client
to commit a transaction writing to $\key$ if and only if its view
contains all versions available in the global state for $\key$.  This
prevents $\cl_2$ from running $\ctrinc(\key)$ on
\cref{fig:counter_kv_first_inc} if its view only contains the initial
version of $\key$.  Instead, the $\cl_2$ view must contain both
versions of $\key$, thus enforcing $\cl_2$ to write a version with
value $2$ after running $\ctrinc(\key)$. This particular test
corresponds to \emph{write-conflict-freedom} of distributed kv-stores:
at most one concurrent transaction can write to a key at any one time.
In \cref{sec:cm} we give many examples of execution tests and their
associated consistency models on kv-stores. In \cref{sec:other_formalisms}, we
develop a proof technique, which we use in \cref{app:et_sound_complete} 
to show the equivalence of our operational definitions of consistency models and the 
declarative ones based on  
execution graphs. 

\mypar{General Robustness Results} 
The first application of our operational semantics is to prove
general robustness results for clients with respect to specific
consistency models (\cref{sec:robustness}). 
Using our operational semantics, we can prove invariant properties
of programs under weak consistency models
such as robustness,
that is, the invariant obtained by executing under a weak consistency model can
be obtained under serialisability.
We prove the robustness of a single
counter against $\PSI$, and the robustness of multiple counters and banking example \cite{bank-example-wsi}
against our new model $\WSI$ and any stronger models such as $\SI$.
The latter is done through general conditions on invariant which guarantees robustness against \( \WSI \).
Thanks to our operational semantics, our invariant-based approaches only need to work with program steps rather than the whole programs.

\mypar{Verifying Implementation Protocols} 
The second application of our operational
semantics is to determine that implementations of distributed
kv-stores satisfy certain consistency models. 
Kv-stores and views provide a 
faithful abstraction of the state of geo-replicated and partitioned
databases, and  execution tests provide a powerful abstraction of the synchronisation mechanisms 
enforced by these databases to commit a transaction. This makes it
possible to use our 
semantics to verify the correctness of distributed database protocols. 
We have shown that the
COPS protocol \citep{cops} for implementing a replicated database satisfies \( \CC \)  (\cref{sec:verify-impl}), 
and the Clock-SI protocol \citep{clocksi} for implementing a
partitioned database satisfies the consistency model called $\SI$ (\cref{sec:clock-si}). 


\sx{
\newpage 

BELOW GOES

However, the situation becomes more complicated when the kv-store contains multiple counters:  
since each client has its own view on the kv-store, and views of clients are independent from each other, it is possible for two 
clients to observe the increments on two distinct counters, $\Counter(\key_1)$ and $\Counter(\key_2)$, in different order. 
For instance, consider the following program:

\vspace{-5pt}
{%
\displaymathfont
\begin{align}
		\cl_0: 
		 \ctrinc(\key_1) ; \ctrinc(\key_2)
		 \;\; || \;\;  \cl_1: 
		 \ctrread(\key_1) ; \ctrread(\key_2)
		  \;\; || \;\;  \cl_2: 
		 \ctrread(\key_1); \ctrread(\key_2)
	\tag{\textsc{LF}}
	\label{prog:LF}
\end{align}	 
}%
Suppose that $\cl_0$ executes first by incrementing $\key_1$, $\key_2$.
This results in $\key_1$ and $\key_2$ having two versions with values $0$ and $1$ each. 
Client $\cl_1$ executes its transactions next, using a view that 
%Let us assume that the view of $\cl_1$ 
contains both versions of $\key_1$, but only 
the initial version of $\key_2$:  client $\cl_1$ then reads $1$ for $\key_1$ and $0$ for $\key_2$; \ie $\cl_1$ observes
%When next client $\cl_1$ executes, it thus reads $1$ for $\key_1$ and $0$ for $\key_2$; 
%that is, 
%from the point of view of $\cl_1$, 
the increment of $\key_1$ 
happening before the increment of $\key_2$. 
Finally, $\cl_2$ executes its transactions using a view that contains both versions for $\key_2$, but only 
the initial version of $\key_1$: 
client $\cl_2$ reads $0$ for $\key_1$ and $1$ for $\key_2$; 
\ie $\cl_2$
%that is, from the point of view of $\cl_2$, 
observes the increment of $\key_2$ 
happening before the increment of $\key_1$. 
This behaviour is known as the \emph{long fork} anomaly (\cref{fig:cp-disallowed}). 

The long fork anomaly is disallowed under strong models, \eg serialisability (\(\SER\)) and snapshot isolation (\(\SI\)), 
but is allowed under weaker models \eg parallel SI (\(\PSI\)) and causal consistency (\(\CC\)). 
To capture such consistency models and rule out the long fork anomaly as a possible result 
of \eqref{prog:LF}, we must strengthen the execution test associated with the kv-store. 
For \(\SER\), we strengthen the execution test by ensuring that a client can execute a transaction 
only if its view contains all versions available in the global state. 
For SI, the candidate execution test recovers the order in which 
updates of versions have been observed by different clients (\eg $\cl_1$), 
and allows a transaction to commit only if the observations made by the committing client (\eg $\cl_2$) are consistent with previous clients (\ie $\cl_1$): we give the formal definition of this execution test  in \cref{sec:cm}.
Under such strengthened execution tests, in the \eqref{prog:LF} example $\cl_2$ cannot
observe $1$ for $\key_2$ after observing $0$ for $\key_1$; 
this is because $\cl_1$ has already established that the increment on $\key_2$ happens after 
the one of $\key_1$. 

%In \cref{sec:program-analysis}, we prove that if the kv-store consists of multiple counter objects, and the execution test employed by transactions guarantees \( \SI \), then the kv-store 
%behaves as is it were (strictly) serialisable.
%As we demonstrate in \cref{sec:cm}, using execution tests on kv-stores, we can define all well-known consistency models (weak or strong) subject to a few basic conditions. 
%Moreover, in \cref{sec:verify-impl} we encode two different distributed protocols using kv-stores and views: COPS \cite{cops}, 
%a geo-replicated protocol for causal consistency, and Clock-SI \cite{clocksi}, a protocol for \(\SI\) under partitioned key-value stores.
%Each of these protocols is verified against our definition using execution tests.

\newpage 
}
