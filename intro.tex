\section{Introduction}
\label{sec:intro}

Transactions are the \emph{de facto} synchronisation mechanism in
modern distributed databases.  To achieve scalability and performance,
distributed databases often use {weak}  transactional consistency
guarantees known as \emph{consistency models}.  Many consistency
models were originally invented by engineers using (some quite
informal) definitions specific to particular real-world reference
implementations
\cite{gdur,ramp,CORFU,tango,si,distrsi,clocksi,redblue,rola,cops,PSI-RA,NMSI,PSI,wren}.
More recently, unified definitions of consistency model have been
defined independently of particular implementations, either
declaratively using execution graphs~\cite{adya,ev_transactions} or
operationally using graph-based or state-based semantics \cite{seebelieve,alonetogether,sureshConcur}.


A key challenge is to define a unified semantics for consistency
models which provides  a {\em mid-point}  with which to 
verify distributed implementations {\em and} analyse the
behaviour of client programs with respect to a particular consistency model. Current
work tends to be suitable  either for verifying implementations or for reasoning
about
clients. As far as we are aware, there has been no
previous work on distributed transactions that does both. 
We introduce an operational semantics for describing the
client-observable behaviour of atomic transactions 
updating distributed key-value stores (kv-stores). With our semantics, we can verify distributed implementations and prove
invariant properties of client programs.  




The declarative approach for defining 
consistency models using execution graphs has been substantially
studied \cite{adya,ev_transactions,framework-concur,SIanalysis,laws}. 
In such graphs,  nodes are atomic transactions and edges describe the
known dependencies between transactions.
They capture different consistency models using  a two-step procedure:
\begin{enumerate*}
\item construct {\em candidate executions} of the whole program comprising
transactions in which reads may contain arbitrary values; and 
\item apply a number of \emph{axioms} on such executions to rule out invalid executions. 
Such axioms may state, for example, that every read is
validated by a write that has written the read value. 
\end{enumerate*}
The most well-known execution graphs are dependency graphs \cite{adya} and abstract
executions \cite{ev_transactions,framework-concur}. 
Dependency graphs tend to be used to analyse  client programs: for
example, 
Fekete et al. \citet{fekete-tods} derived 
a static analysis checker for a particular weak consistency model called
snapshot isolation;  Bernardi and Gotsman \citet{giovanni_concur16}
developed a static analysis checker for several weak consistency
models assuming the so-called snapshot property\footnote{The \emph{snapshot property}, 
also known as \emph{atomic visibility}, means that
the client observes that a transaction reads from an atomic snapshot
of the database, commits atomically and intermediate states are not observable to other clients, 
even if the underlying
distributed protocol has a more fine-grained behaviour.}; and 
Beillahi et al. \citet{snapshot-isolation-robust-tool} developed a tool based on Lipton's reduction \cite{Lipton-reduction}
for checking robustness properties against snapshot isolation: 
that is, a  particular program (or set of programs) behaves as if the consistency model is strong consistency. 
Abstract executions, on the other hand, tend to be used to verify  implementation protocols: for example,
abstract executions are the  standard by which many system engineers
demonstrate that their protocols satisfies  certain
consistency models \cite{cops,NMSI,PSI}
Execution graphs provide little information about how the 
state evolves throughout the execution of a program.
They are unsuitable for invariant-based program analysis 
which requires reasoning about the step-wise execution of a program. 

\pg{The footnote on snapshot property can be imporved.}

\pg{I am worried about Nagar and Jagannathan. Have they shown that
  robustness and implementations can be done in the same semantics,
  then what I'm saying earlier is wrong although what I'm saying
  earlier is what we have before. \sx{NO, they only did robustness}}

The operational approach has been less studied
\cite{seebelieve,alonetogether,sureshConcur}.  Nagar and
Jagannathan \citet{sureshConcur} introduced an operational semantics
for abstract execution graphs, and
prove the robustness of client programs using model checking. Each
execution step adds a new transaction node in the graph by first
constructing candidate steps and then applying axioms to the whole
graph to rule out invalid candidate steps.  They focus on consistency models with the snapshot property, but confusingly allow 
the interleaving of fine-grained operations between transactions. 
This results in an unnecessary explosion of the space of traces obtained by 
the program.
This approach is unlikely
to be suitable for invariant-based analysis associated with the
state.\pg{I currently find this on Nagar and Jagannathan  bit really weak.}
Kaki et al. \citet{alonetogether} proposed an operational semantics for
SQL transaction programs over an abstract centralised store with the consistency
models given by the standard ANSI/SQL isolation levels \cite{si}. They
develop a program logic and prototype tool for reasoning about client
programs, and so can capture invariant properties of the state. They can
express snapshot isolation \cite{si}, but the consensus is
that they cannot
capture the weaker consistency models such as parallel snapshot isolation \cite{PSI} 
important for distributed databases. 
%\pg{Do we need any of this:
%Kaki et al. \citet{alonetogether} propose an operational semantics of SQL transactional programs 
%under the consistency models given by the standard ANSI/SQL isolation levels \cite{si}.
%In their  framework, transactions work on a local copy of the global state 
%of the system, and the local effects of a transaction are committed to the  
%system state when it terminates. Because state changes 
%are made immediately available to all clients of a system, this model 
%is not suitable to capture weak consistency models such as \(\PSI\) or \(\CC\). 
%They introduce a program logic and prototype verification tool for reasoning 
%about client programs. \sx{The above is a good summary.} }
Crooks et al. \citet{seebelieve} provided a trace semantics
over a global centralised store, where an execution step involves
analysing the totally-ordered
history of states and the read-write set of the transaction.
They show the equivalence of several
implementation-specific definitions of consistency model. However, their
approach does not lend itself to  analysing client programs,
since observations made by clients require the total order in
which transactions commit which is not realistic for clients. 
Koskinen and Parkinson \citet{push-pull}
provided a log-based abstract operational semantics for verifying several implementations of serilisability.
However, it is unknown that they could be easily extended to tackle weaker consistency models.


Crooks et al. \citet{seebelieve} propose a state-based formal framework for weak consistency models 
that employs concepts  similar to execution tests and views, called commit tests and read states respectively.
They prove that consistency models previously thought to be different are in fact equivalent in their semantics. 
They capture a wide range of consistency models including read committed which we cannot do. 
In their semantics, one-step trace reduction is determined by the whole previous history of the trace. 
In contrast, our reduction step only depends on the current configuration (kv-store and view).
They do not consider program analysis. Their notion of commit tests and read states requires 
the knowledge of information that is not known to clients of the system, i.e. the total order of system changes that happened in the database 
prior to committing a transaction. For this reason, we believe that
their framework is not suitable for the development of techniques for analysing client programs. 


%\pg{This seems like something to put in conclusions, saying it would
  %be really good to see how the two worlds merge. Doherty et al. \citet{op-semantics-c11-rar} develop an operational semantics for release-acquire fragment of
%C11 memory model, a variant of causal consistency.
%Their semantics is based on a variant of dependency graph where nodes and edges 
%are tailored for C11 operations.
%They introduce per-thread observations that are compatible for executing next operations;
%this is similar to our views and execution tests.
%We believe we can model release-acquire fragment of C11.}



%\pg{This should go in section 3 where we define our kv-stores and
  %views. 
%Koskinen et al. \citet{log-based-op} and Koskinen and Parkinson
%\citet{push-pull}  proposed log-based semantics for 
%verifying implementations that satisfy strong consistency, based on kv-stores
%and also other ADTs. Their work comprises a centralised global log 
%and partial client local logs, which is a little similar to 
%our kv-stores and views. Their model focusses on strong consistency, and 
%there is no evidence that they could be easily extended to tackle
%weaker consistency models.
%}



\pg{Not done from here.} 
We introduce an \emph{interleaving} operational semantics for
describing the client-observable behaviour of atomic transactions on
distributed key-value stores (\cref{sec:model}). Our semantics uses abstract states
comprising \emph{a global, centralised key-value store} (kv-store) with
\emph{multi-versioning}, which records all the versions of a key, and
\emph{partial client views}, which let clients see only a subset of the
versions.  Our client views partly inspired by the views in the C11
operational semantics in~\cite{promises}.  An execution step depends
simply on the abstract state, the read-write set of the transaction, and an \emph{execution test} which
determines if a client with a given view is allowed to commit a
transaction. Different execution tests give rise to different consistency models, 
which we show to be equivalent to well-known
declarative definitions of consistency model on abstract executions (%
\ifTechRepEdits%
technical report%
\else%
\cref{app:et_sound_complete}%
\fi%
).
Our execution tests resemble the  approach taken in~\cite{seebelieve},
except that they require an analysis of the whole trace for an
execution step whereas we require the current abstract state. 




We make the assumption that our transactions satisfy the \emph{snapshot property}, 
also known as \emph{atomic visibility}. This means that
the client observes that a transaction reads from an atomic snapshot
of the database, commits atomically and intermediate states are not observable to other clients, 
even if the underlying
distributed protocol has a more fine-grained behaviour. This
assumption is reasonable in distributed databases: for example, with
on-line shopping application, a client only sees one snapshot of the database and
only has knowledge that their transaction has successfully committed.
We also make the assumption that our transactions satisfy the \emph{last-write-wins} resolution policy,
a widely used policy in many real-world distributed key-value stores. 
This means that when a transaction observes several updates to a key,
the atomic snapshot contains the value written by the last update.
Our execution tests  uniformly capture  the well-known consistency models (\cref{sec:cm}) such as,
\emph{causal consistency} \((\CC)\) \citep{ev_transactions,cops,causal-def}, 
\emph{parallel snapshot isolation} \( (\PSI) \) \citep{NMSI,PSI},
\emph{snapshot isolation} \((\SI)\) \citep{si} 
and \emph{serialisability} \((\SER)\) \citep{Papadimitriou-ser}.
We also identify a new consistency model, called \emph{weak snapshot isolation} (\(\WSI)\), 
that sits between \(\PSI\) and \(\SI\) and retains good properties of both.

Using our operational semantics, we demonstrate that we can verify
that database protocols satisfy particular consistency models and
prove invariant properties of client programs with respect to such
consistency models (\cref{sec:applications,sec:program-analysis}).
In particular, we establish the correctness of two database
protocols: the COPS protocol for the fully replicated kv-stores \cite{cops} 
which satisfies causal consistency (\cref{sec:verify-impl}); 
and the Clock-SI protocol for partitioned kv-stores \cite{clocksi} 
which satisfies snapshot isolation 
(%
\ifTechRepEdits%
technical report%
\else%
\cref{app:implementation-verification}%
\fi%
). 
We prove invariant properties of client programs including general robustness results
for client programs with certain patterns of transaction and
invariant properties of specific programs not satisfying 
such robustness results (\cref{sec:invariant-client-programs}). 
We believe out robustness results are the first to take into account client
sessions: with sessions, we show that multiple counters {\em are not} robust against \(\PSI\);
interestingly, without sessions, \citet{giovanni_concur16} show that multiple counters {\em are}
robust against \(\PSI\) using static-analysis techniques which are
known not to be applicable to sessions.  
We show the correctness of a lock paradigm under \emph{update atomic} (\( \UA \)),
a consistency model disallowing conflict writes to a key,
although this lock is not robust against \( \UA \).

With our operational semantics, we believe that we have identified an interesting \emph{mid-point}
between distributed databases and clients.
As far as we are aware, there has been no previous work on transactions
that, in the same general semantics, verifies distributed protocols and analyses client programs. 
This was an important goal for us, resonating with recent work
that does just this for standard shared-memory concurrency \cite{tada,cap,iris,fcsl}. 

